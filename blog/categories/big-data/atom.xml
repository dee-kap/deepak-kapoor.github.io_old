<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Big Data | Deepak Kapoor]]></title>
  <link href="http://www.deepakkapoor.net/blog/categories/big-data/atom.xml" rel="self"/>
  <link href="http://www.deepakkapoor.net/"/>
  <updated>2013-12-09T21:38:22+11:00</updated>
  <id>http://www.deepakkapoor.net/</id>
  <author>
    <name><![CDATA[Deepak Kapoor]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Hadoop Word Count Revised]]></title>
    <link href="http://www.deepakkapoor.net/2012/07/11/hadoop-word-count-revised/"/>
    <updated>2012-07-11T00:00:00+10:00</updated>
    <id>http://www.deepakkapoor.net/2012/07/11/hadoop-word-count-revised</id>
    <content type="html"><![CDATA[<p>This is a follow up to my <a href="http://www.debugrelease.com/yet-another-hadoop-word-count-tutorial/">last post</a> in which I showed you how to write a word count MapReduce job. Have a look at <a href="http://www.debugrelease.com/yet-another-hadoop-word-count-tutorial/">that earlier post</a> before reading on. It will put things into perspective. </p>


<p> <p>As you saw we implemented our map and reduce methods in their own classes by extending Mapper and Reducer class from Hadoop framework. It turns out that there is a way to implement a word count example without doing that. Hadoop framework already ships with two classes which can be used as our mapper and reducer. They are TokenCounterMapper from org.apache.hadoop.mapreduce.lib.map package and IntSumReducer from org.apache.hadoop.mapreduce.lib.reduce package. </p> <p>Here is a revised word count which uses built-in TokenCounterMapper and IntSumReducer classes.</p></p>

<pre class="lang:java decode:true " >
package com.thereforesystems.hadoop;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.map.TokenCounterMapper;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.mapreduce.lib.reduce.IntSumReducer;
import org.apache.hadoop.util.GenericOptionsParser;

public class HadoopWordCountRevised {

    public static void main(String[] args) throws Exception {
        
        Configuration config = new Configuration();
        String[] otherArgs = 
            new GenericOptionsParser(config, args).getRemainingArgs();
        
        Job job = new Job(config, "Word Count Tutorial");
        job.setJarByClass(HadoopWordCountRevised.class);
        job.setMapperClass(TokenCounterMapper.class);
        job.setReducerClass(IntSumReducer.class);
        
        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(IntWritable.class);
        
        FileInputFormat.addInputPath(job, new Path(otherArgs[0]));
        FileOutputFormat.setOutputPath(job, new Path(otherArgs[1]));
        
        System.exit(job.waitForCompletion(true) ? 0 : 1);
    }
}
</pre>

]]></content>
  </entry>
  
</feed>
